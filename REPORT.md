# Report

## Learning Algorithm

To solve the single arm reacher environment, a variant of a Deep Deterministic Policy Gradient (DDPG) is implemented. The idea is based on the paper [Continuous control with deep reinforcement learning]([https://](https://arxiv.org/abs/1509.02971)). It's well suited for learning continuous action spaces. It combines elements from both policy gradient methods and Q-learning.

The key ideas of the approach are:

- Actor-Critic Architecture: DDPG employs two neural networks: the actor, which determines the best action to take given a state, and the critic, which evaluates the action taken by the actor by estimating the Q-value.
- Deterministic Policy: Unlike stochastic policy methods, DDPG uses a deterministic policy, meaning the actor outputs a specific action for a given state rather than a probability distribution over actions.
- Experience Replay: DDPG utilizes a replay buffer to store past experiences (state, action, reward, next state) and samples from this buffer to break the correlation between consecutive experiences, improving training stability.
- Target Networks: To stabilize training, DDPG employs target networks for both the actor and critic. These networks are updated slowly using a technique called soft updates, which helps to mitigate oscillations during training.
- Continuous Action Spaces: DDPG is particularly suited for environments with continuous action spaces, making it effective for tasks like robotic control.

## Enhancements to the original algorithm

A prioritized replay buffer with beta annealing for importance sampling was implemented. This assigns a higher sampling probability to samples with a large absolute TD-error. The beta annealing adds a control mechanism to not over exploit good training examples in the beginning of the learning phase. Then it gradually shifts to more exploitation in the later phase of the learning.

Another mechanism to emphasize exploration at the beginning of the training was to define a number of episodes which are solely used to gather training data. In this data collection episodes no learning is done.

Final measure taken to control the exploration of the state space was the addition of an Adaptive Ornstein-Uhlenbeck process. It adds decaying noise to the actions generated by the agent's actor. It effectively reduces the noise scale during the process of learning, leading to more exploration in the beginning and more knowledge exploitation later.

To reduce the variance in the Q-value estimates, N-step returns have been implemented. They are effectively integrated into the prioritized replay buffer.

## Architecture of used neural networks

In this implementation the actor has an 33 dimensional input layer which corresponds to the observation space of the reacher environmemt. The first hidden layer has 400 dimensionas, followed by a second hidden layer of 300 dimensions. Up to here, every hidden layer is followed  by Rectified Linera Units (ReLUs). The output layer has 4 dimensions and is followed by a tangens hyperbolicus activation function to bring the control values of the output layer into the range of -1 to +1.

The critic has a very similar architecture. It also has a 33 dimensional input layer. This is then connected to a (400+4) dimensional hidden layer in which the 4 control values from the actor's output are injected. This is followed by a ReLU. The second hidden layer has again 300 dimensions and is followed by a ReLu, too. The output is of dimension 1 which resembles the approximated q-value which the critic assignes to the observations and the actions selected by the actor.

## Hyperparameter choosen

The parameter are sorted by the files in which they are set:

**./code/ddpg_train.py** (main entry point for learning the agent as descibed in the readme):

- Maximum number of episodes to train (max_episodes_count): 5000
- Maximum number of time steps per episode (max_steps_count): 999
- Number of episodes in the beginning solely used for data collection and no training (pure_data_collection_episodes_nb): 30
- Frequency of learning steps (learn_every_x_steps): 5
- Number of learing iterations when the agent is in a learning step(learning_steps): 10
- Length of moving average for calculating its performace (score_window_size):  100
- Target score to achieve over the moving average of 100 episodes (target_score): 30.0

**./code/ddpg_architectures.py**:

See above paragraph *Architecture of used neural networks* for detailed description of the neural networks parameter.

**./code/ddpg_agent.py**:

- Learning rate of the actor (lr_actor): 1e-4
- Learning rate of the critic (lr_critic): 3e-4
- Discount factor (gamma): 0.98
- Soft update factor (tau): 1e-3
- Size of prioritized replay buffer (buffer_size): 1e6
- Number of samples in each batch for training (batch_size): 128
- Number of N-Steps for return calculation (n_steps): 5
- Mean initial noise value (mu): 0.0
- Scale factor to bring noise back to zero/mu (theta): 0.15
- Noise scale at the beginning (sigma_start): 0.3
- Noise scale at the end (sigma_end): 0.05
- Noise decay factor for each step (sigma_decay): 0.99999
- Weight decay for the critic's optimizer (WEIGHT_DECAY): 0.0001

**./code/prioritized_replay_buffer.py**:

- Priority exponent parameter (alpha): 0.6
- Importance sampling exponent parameter (beta): 0.4
- Final value of beta (beta_end): 1.0
- Number of steps to anneal beta (beta_frames): 100000  

## Plot of rewards

Given these hyperparameter the implemented agent solved the environment after 511 episodes, where it reached an averaged score of +30.06  over 100 episodes:

[Training progress](./results/DDPG_Training_episode_idx_000511.png)

## Network weights of agent solved the environment

The correspondig network weights can be found here: [Saved Network Weights](./results/solved_agent.pth)

## Ideas for future work

First option could be a more sophisticated control of the learning parameter. My subjective impression was that the speed of learning could have been improved a lot by carefully observing the progess during learning. While the importance sampling with beta annealing of the replay buffer and the adaptive noise model of the agent all reduce the exploration over time, they do it without any relation to the agent's actuel speed of convergence.

In the same way, the number of learning steps and the update frequency could be made adaptive and set into relation to the agent's current performance.

Considering the replay buffer:

- Implementation of a tree based replay buffer to reduce sampling complexity from O(n) to O(log(n)).
- Complete tensor approach for the replay buffer without or as few python lists or numpy arrays as possible.

Algorithmic variant:

- Implementing a Distributed Distributional Deep Deterministic Policy Gradient algorithm presented in [D4PG]([https://](https://openreview.net/forum?id=SyZipzbCb)). It's main advantage over DDPG seems to be that it models the distribution of returns rather than just the expected return as a single value. This can capture more information about the uncertainty and variability of outcomes and could lead to better stability and performance of the training.
